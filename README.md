# MoMA: Multimodal LLM Adapter for Fast Personalized Image Generation

Arxiv page: [paper](https://arxiv.org/abs/2404.05674)

## test page. This repository is ðŸ”´ðŸŸ¡Under ConstructionðŸŸ¡ðŸ”´.


This is the [Official] repository of MoMA: MLLM Adapter. 

You would need to 
+ install LlaVA from its [official repository](https://github.com/haotian-liu/LLaVA).
+ download fune tuned MLLM checkpoint (13G) (uploading)
+ download the checkpoint for attention adapters [here](https://drive.google.com/file/d/1jLg77aGK3X7X7krQ-yV-eNw5wXApupQo/view?usp=sharing)


Use run_evaluate_MoMA.py as an entrance. 
