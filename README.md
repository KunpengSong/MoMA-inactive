# MoMA: Multimodal LLM Adapter for Fast Personalized Image Generation

## test page. This repository is Under Construction.

This is the [Official] repository of MoMA: MLLM Adapter. 

You would need to 
+ install LlaVA from its [official repository](https://github.com/haotian-liu/LLaVA).
+ download fune tuned MLLM checkpoint (13G)
+ download the checkpoint for attention adapters [here](https://drive.google.com/file/d/1jLg77aGK3X7X7krQ-yV-eNw5wXApupQo/view?usp=sharing)


Use run_evaluate_MoMA.py as an entrance. 
